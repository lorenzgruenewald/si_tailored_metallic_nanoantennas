{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0968b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client, progress\n",
    "from osirisprocess import OsirisProcess\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b07dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the list of dask workers setted up in different nodes [node1-node4]\n",
    "client0 =  Client(\"NODE1:8786\")\n",
    "client1 =  Client(\"NODE2:8786\")\n",
    "client2 =  Client(\"NODE3:8786\")\n",
    "client3 =  Client(\"NODE4:8786\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b62098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the clients\n",
    "\n",
    "clients = [client0,client1,client2,client3]\n",
    "[i.restart() for i in clients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91fdf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the DATA PATH\n",
    "basepath= \"/OSIRIS_SIMULATION_DATA_PATH\"\n",
    "# Base name, all the simulation contained in the basepath (Same shape, but Length/Radii of antenna changed = SCAN) start with the same name\n",
    "basename = f\"{basepath}/SCAN_NAME\"\n",
    "\n",
    "# Function which is send to each dask worker, just take the argument to simulation\n",
    "# to be processed\n",
    "def run(path):\n",
    "    import sys\n",
    "    # Add the path where the OsirisProcess module is located\n",
    "    sys.path.insert(0, 'OSIRIS_PROCESS_MODULE_PARENT_DIRECTORY_PATH') \n",
    "    from osirisprocess import OsirisProcess\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Depending on what you want to comment or uncomennt the lines you want.\n",
    "    s = OsirisProcess(path,1)\n",
    "    s.convert_h5_to_zarr() #CONVERT FROM HDF5 TO ZARR\n",
    "    #s.load_fields_zarr() \n",
    "    #s.convert_to_cyl()\n",
    "    #max_values = s.find_max_values()\n",
    "    #df = pd.read_pickle(\"datasets/SCAN_FILE.pckl\")\n",
    "    #name = path.split(basepath)[1]\n",
    "    #max_values = {path: df.loc[name].to_dict()}\n",
    "    #s.set_max_values(max_values)\n",
    "    #return max_values\n",
    "\n",
    "# Store all the simulation paths \n",
    "paths = [i for i in sorted(glob.iglob(f\"{basename}*\"))] \n",
    "# Split the list of paths in 4 parts, one for each dask client (CLIENT0-CLIENT3)\n",
    "split_paths  = [paths[start::len(clients)] for start in range(4)]\n",
    "# Calculate the lengths of paths\n",
    "lengths = np.array([len(i) for i in split_paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6172e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main cell, which will process batchs of 4 simulations in parallel.\n",
    "\n",
    "results = []\n",
    "for i in range(lengths.max()):\n",
    "    if i < lengths[0]:\n",
    "        print(split_paths[0][i])\n",
    "        t1 = client1.submit(run,split_paths[0][i])\n",
    "    if i < lengths[1]:\n",
    "        print(split_paths[1][i])\n",
    "        t2 = client2.submit(run,split_paths[1][i])\n",
    "    if i < lengths[2]:\n",
    "        print(split_paths[2][i])\n",
    "        t3 = client3.submit(run,split_paths[2][i])\n",
    "    if i < lengths[3]:\n",
    "        print(split_paths[3][i])\n",
    "        t4 = client4.submit(run,split_paths[3][i])\n",
    "    try:\n",
    "        if i < lengths[0]:\n",
    "            results.append(t1.result())\n",
    "    except:\n",
    "        results.append({f\"c0_{i}\": \"error\"} if i < lengths[0] else None)\n",
    "    try:\n",
    "        if i < lengths[1]:\n",
    "            results.append(t2.result())\n",
    "    except:\n",
    "        results.append({f\"c1_{i}\": \"error\"} if i < lengths[1] else None)\n",
    "    try:\n",
    "        if i < lengths[2]:\n",
    "            results.append(t3.result())\n",
    "    except:\n",
    "        results.append({f\"c2_{i}\": \"error\"} if i < lengths[2] else None)\n",
    "    try:\n",
    "        if i < lengths[3]:\n",
    "            results.append(t4.result())\n",
    "    except:\n",
    "        results.append({f\"c3_{i}\": \"error\"} if i < lengths[3] else None)\n",
    "    client1.restart()\n",
    "    client2.restart()\n",
    "    client3.restart()\n",
    "    client4.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229e9968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we calculate the max values. This converts the results to a pandas dataframe and store into file.\n",
    "\n",
    "key_names = [list(i.keys())[0].split(basepath)[1] for i in results[:-3]]\n",
    "sort_key = lambda x: [float(x.split(\"SPLIT_MARKER\")[-1]) for  i in x] #THE SPLIT MARKER SHOULD SEPARATE THE COMMON FIRST PART OF ALL SIMULATION DIRECTORIES WITHIN THE SCAN AND THE SCAN PARAMETER IN THE LAST PART\n",
    "index_names = [list(i.keys())[0].split(basepath)[1] for i in results[:-3]]\n",
    "index_names = sorted(list(dict.fromkeys(index_names)),key=sort_key)\n",
    "results_df = [pd.DataFrame(i[list(i.keys())[0]]) for i in results[:-3]]\n",
    "final_df = pd.concat(results_df,keys=key_names)\n",
    "\n",
    "sorted_df_list = [final_df.loc[i] for i in index_names]\n",
    "sorted_df = pd.concat(sorted_df_list,keys=index_names,sort=False)\n",
    "sorted_df.to_pickle(\"datasets/SCAN_FILE.pckl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (osiris_env)",
   "language": "python",
   "name": "osiris_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
